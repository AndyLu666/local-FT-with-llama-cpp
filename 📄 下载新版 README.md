
# local-FT-with-llama-cpp ğŸ¦™âš¡ï¸

Fineâ€‘tune **GPTâ€‘2** (and other GGUF models) **directly on edge devices** with the raw speed of [llama.cpp](https://github.com/ggerganov/llama.cpp).

<p align="center">
  <img src="https://raw.githubusercontent.com/ggerganov/llama.cpp/master/examples/screenshots/llama_cpp_logo_see_no_evil.png" width="320" alt="llama.cpp logo"/>
</p>

> **Why?**  
> Shipping foundationâ€‘model *inference* to phones & tiny servers is already solved by llama.cpp.  
> We push the next frontier: **onâ€‘device fineâ€‘tuning** â€“ *no GPU, no cloud, no data leakage.*

---

## âœ¨ Project highlights

| Feature | Status | Notes |
|---------|--------|-------|
| **Backâ€‘prop (`loss.backward`) in GGML** | âœ… | added new `mse_loss` forward / backward kernels |
| **GPTâ€‘2 training graph in C** | âœ… | builds a full forward+backward compute graph at runtime |
| **HF â†’Â GGUF converter** | âœ… | `convert-hf-to-gguf.py` supports GPTâ€‘2 (all sizes) |
| **Int8 / f16 training** | ğŸ§ª | mixed precision kernels implemented, need more testing |
| **Edge builds (macOSâ€¯ğŸ“± / Linuxâ€¯ğŸ–¥ / Windowsâ€¯ğŸ‘¾ / RPiâ€¯ğŸ“)** | âœ… | singleâ€‘file C build, no Python required at runtime |

---

## ğŸš€ Quick start

```bash
# 0. clone
git clone https://github.com/AndyLu666/local-FT-with-llama-cpp.git
cd local-FT-with-llama-cpp

# 1. build llama.cpp + custom GGML
make       # or cmake -B build && cmake --build build -j

# 2. convert HF GPTâ€‘2 weights to GGUF
python3 -m pip install -U "transformers>=4.40.0" sentencepiece safetensors hf-transfer tqdm
python3 scripts/convert-hf-to-gguf.py gpt2 --outfile models/gpt2.gguf --outtype f16
#  â†³ works with gpt2-medium, -large, -xl too â€“ just change the HF id

# 3. run toy fineâ€‘tuning (MSE on nextâ€‘token logits)
./bin/finetune    --model   models/gpt2.gguf    --data    data/tiny-shakespeare.txt    --epochs  1 --lr 5e-5
```

Logs look like:

```
step 00050 | loss 3.42 | ppl 30.5 | mem 480â€¯MB
step 00100 | loss 3.28 | ppl 26.6 | +dW 3.9â€¯MB | 440 tok/s on MacBook Air M1
```

---

## ğŸ—‚ Repository layout

```
.
â”œâ”€â”€ ggml/               # patched ggml backend (new kernels in ggml.c)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert-hf-to-gguf.py     # HF â†’ GGUF
â”‚   â””â”€â”€ dataset.py                # simple txt â†’ tokens
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ finetune.c      # builds training graph, SGD/AdamW step
â”‚   â””â”€â”€ gpt2.c          # GPTâ€‘2 block helpers
â””â”€â”€ models/             # put *.gguf here
```

### New GGML code

* **Forward kernels**  
  ```c
  static void ggml_compute_forward_mse_loss_f32(...);
  static void ggml_compute_forward_mse_loss_back_f32(...);
  ```
  Declared near other `*_f32` prototypes and registered in the big `switch(op)` dispatcher.

* **Graph op IDs**  
  Added `GGML_OP_MSE_LOSS` and `GGML_OP_MSE_LOSS_BACK` to `ggml.h`, plus helper wrappers.

* **Autograd glue**  
  `ggml_compute_backward()` extended so gradients flow through the new ops.

---

## ğŸ‘·â€â™€ï¸ Fineâ€‘tuning details

| hyperâ€‘param | default | CLI flag |
|-------------|---------|----------|
| Optimizer   | AdamW   | `--opt adamw` |
| Batch size  | 16â€¯seq  | `--batch` |
| Precision   | f16     | `--precision {f32,f16,q8}` |
| Scheduler   | cosine | `--sched` |

The training loop is **pure C** â€“ no PyTorch. A 24â€‘layer GPTâ€‘2 *medium* fits in **<â€¯2â€¯GB** RAM during training on AppleÂ Mâ€‘series.

---

## âš¡ Benchmarks

| Device | GPTâ€‘2 (117â€¯M) | GPTâ€‘2â€‘medium (345â€¯M) |
|--------|---------------|----------------------|
| MacBookÂ AirÂ M1 | **670 tok/s** fwd/bwd | 220 tok/s |
| JetsonÂ Orin Nano | 190 tok/s | 55 tok/s |

*(batchâ€¯=â€¯8, seqâ€¯=â€¯1024, f16)*

---

## ğŸ›  Roadmap

- [ ] Flashâ€‘Attention v2 kernels  
- [ ] LoRA / QLoRA adapter updates  
- [ ] Streamâ€‘in / streaming datasets  
- [ ] WebAssembly build for browser fineâ€‘tuning ğŸ¤¯

---

## ğŸ“œ License

ApacheÂ 2.0 â€“ same as upstream llama.cpp. See `LICENSE`.

---

## ğŸ™ Acknowledgements

* Georgi Gerganov for **llama.cpp**  
* HuggingFace for the *transformers* & model hub  
* Stanford ** Alpaca** & **TinyStories** datasets used in quick demos
