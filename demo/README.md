# LLAMA.cpp ç¥ç¶“ç¶²è·¯è¨“ç·´æ¼”ç¤º

é€™å€‹æ¼”ç¤ºé …ç›®å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ GGML åº«é€²è¡Œç¥ç¶“ç¶²è·¯è¨“ç·´ï¼Œæ”¯æŒ CPU å’Œ GPU åŠ é€Ÿã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- âœ… **å¤šå¾Œç«¯æ”¯æ´**: CPUã€CUDAã€Metal
- âœ… **æ¨¡çµ„åŒ–è¨­è¨ˆ**: å‰å‘å‚³æ’­ã€åå‘å‚³æ’­ã€å„ªåŒ–å™¨åˆ†é›¢
- âœ… **GPUåŠ é€Ÿ**: è‡ªå‹•æª¢æ¸¬ä¸¦ä½¿ç”¨å¯ç”¨çš„GPU
- âœ… **æ€§èƒ½æ¸¬è©¦**: è©³ç´°çš„æ€§èƒ½åŸºæº–æ¸¬è©¦
- âœ… **æ¨¡å‹ä¿å­˜/åŠ è¼‰**: å®Œæ•´çš„æ¨¡å‹æŒä¹…åŒ–æ”¯æ´

## ğŸ“‹ ç³»çµ±éœ€æ±‚

### åŸºæœ¬éœ€æ±‚
- CMake 3.12+
- C++17 ç·¨è­¯å™¨
- å·²ç·¨è­¯çš„ GGML åº«

### GPU æ”¯æ´éœ€æ±‚

#### CUDA (NVIDIA GPU)
```bash
# Ubuntu/Debian
sudo apt install nvidia-cuda-toolkit

# æˆ–ä¸‹è¼‰ä¸¦å®‰è£ CUDA Toolkit
# https://developer.nvidia.com/cuda-downloads
```

#### Metal (Apple GPU)
- macOS 10.13+
- æ”¯æ´ Metal çš„ Apple è¨­å‚™

## ğŸ”§ ç·¨è­¯æŒ‡å—

### 1. ç·¨è­¯ GGML åº«

é¦–å…ˆéœ€è¦ç·¨è­¯ GGML åº«ï¼š

```bash
cd ggml
mkdir build && cd build

# CPU ç‰ˆæœ¬
cmake ..
make -j$(nproc)

# CUDA ç‰ˆæœ¬
cmake -DGGML_USE_CUDA=ON ..
make -j$(nproc)

# Metal ç‰ˆæœ¬ (macOS)
cmake -DGGML_USE_METAL=ON ..
make -j$(nproc)
```

### 2. ç·¨è­¯æ¼”ç¤ºç¨‹åº

```bash
cd demo
mkdir build && cd build

# CPU ç‰ˆæœ¬
cmake ..
make -j$(nproc)

# CUDA ç‰ˆæœ¬
cmake -DGGML_USE_CUDA=ON ..
make -j$(nproc)

# Metal ç‰ˆæœ¬ (macOS)
cmake -DGGML_USE_METAL=ON ..
make -j$(nproc)
```

## ğŸ® ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬æ¼”ç¤ºç¨‹åº

```bash
# ç°¡å–® MLP (æ‰‹å‹•å¯¦ç¾)
./bin/simple-mlp

# å¢å¼· MLP (æ¨¡çµ„åŒ–è¨­è¨ˆ)
./bin/enhanced-mlp

# ä½¿ç”¨ src çµ„ä»¶çš„ç‰ˆæœ¬
./bin/enhanced-mlp-with-src
```

### GPU åŠ é€Ÿæ¼”ç¤º

```bash
# CPU åŸ·è¡Œ
./bin/gpu-neural-demo

# GPU åŸ·è¡Œ (è‡ªå‹•æª¢æ¸¬)
./bin/gpu-neural-demo --gpu

# æŒ‡å®š CUDA å¾Œç«¯
./bin/gpu-neural-demo --gpu --backend cuda

# æŒ‡å®š Metal å¾Œç«¯ (macOS)
./bin/gpu-neural-demo --gpu --backend metal

# æŒ‡å®š GPU è¨­å‚™
./bin/gpu-neural-demo --gpu --device 0

# é¡¯ç¤ºå¹«åŠ©
./bin/gpu-neural-demo --help
```

## ğŸ“Š æ€§èƒ½æ¯”è¼ƒ

### å…¸å‹æ€§èƒ½æ•¸æ“š (åƒ…ä¾›åƒè€ƒ)

| å¾Œç«¯ | å–®æ¨£æœ¬å‰å‘å‚³æ’­ | æ‰¹é‡å‰å‘å‚³æ’­ (200æ¨£æœ¬) | è¨˜æ†¶é«”ä½¿ç”¨ |
|------|---------------|----------------------|-----------|
| CPU  | ~50-100 Î¼s    | ~10-20 ms           | ~1-2 MB   |
| CUDA | ~20-50 Î¼s     | ~5-10 ms            | ~2-4 MB   |
| Metal| ~30-60 Î¼s     | ~7-15 ms            | ~2-3 MB   |

*å¯¦éš›æ€§èƒ½å–æ±ºæ–¼ç¡¬é«”é…ç½®å’Œæ¨¡å‹å¤§å°*

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### æ ¸å¿ƒçµ„ä»¶

```
src/
â”œâ”€â”€ simple-neural-model.h/cpp    # GPUæ”¯æ´çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹
â”œâ”€â”€ llama-forward.h/cpp          # å‰å‘å‚³æ’­çµ„ä»¶
â”œâ”€â”€ llama-backward.h/cpp         # åå‘å‚³æ’­çµ„ä»¶
â””â”€â”€ llama-optimizer.h/cpp        # å„ªåŒ–å™¨çµ„ä»¶
```

### GPU å¾Œç«¯æ¶æ§‹

```
GGML å¾Œç«¯ç³»çµ±
â”œâ”€â”€ CPU å¾Œç«¯ (ggml-cpu)
â”œâ”€â”€ CUDA å¾Œç«¯ (ggml-cuda)
â”œâ”€â”€ Metal å¾Œç«¯ (ggml-metal)
â””â”€â”€ çµ±ä¸€æ¥å£ (ggml-backend)
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. GGML åº«æœªæ‰¾åˆ°
```
CMake Error: GGML library not found
```
**è§£æ±ºæ–¹æ¡ˆ**: ç¢ºä¿å…ˆç·¨è­¯ GGML åº«
```bash
cd ggml && mkdir build && cd build && cmake .. && make
```

#### 2. CUDA æ”¯æ´æœªå•Ÿç”¨
```
CUDA requested but CUDA toolkit not found
```
**è§£æ±ºæ–¹æ¡ˆ**: å®‰è£ CUDA Toolkit ä¸¦è¨­ç½®ç’°å¢ƒè®Šæ•¸
```bash
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

#### 3. Metal æ”¯æ´å•é¡Œ (macOS)
```
Metal requested but ggml-metal library not found
```
**è§£æ±ºæ–¹æ¡ˆ**: ç¢ºä¿åœ¨ macOS ä¸Šç·¨è­¯ä¸¦å•Ÿç”¨ Metal æ”¯æ´
```bash
cmake -DGGML_USE_METAL=ON ..
```

#### 4. è¨˜æ†¶é«”ä¸è¶³
```
Failed to allocate GPU memory
```
**è§£æ±ºæ–¹æ¡ˆ**: 
- æ¸›å°‘æ‰¹æ¬¡å¤§å°
- ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
- æª¢æŸ¥ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

### èª¿è©¦æŠ€å·§

#### å•Ÿç”¨è©³ç´°æ—¥èªŒ
```bash
export GGML_LOG_LEVEL=DEBUG
./bin/gpu-neural-demo --gpu
```

#### æª¢æŸ¥ GPU ç‹€æ…‹
```bash
# NVIDIA GPU
nvidia-smi

# æª¢æŸ¥ CUDA å®‰è£
nvcc --version
```

## ğŸ“š é€²éšä½¿ç”¨

### è‡ªå®šç¾©æ¨¡å‹åƒæ•¸

```cpp
SimpleNeuralModel::ModelParams params;
params.input_size = 8;      // è¼¸å…¥ç¶­åº¦
params.hidden_size = 32;    // éš±è—å±¤ç¶­åº¦
params.output_size = 5;     // è¼¸å‡ºç¶­åº¦
params.use_gpu = true;      // å•Ÿç”¨ GPU
params.backend_type = "cuda"; // æŒ‡å®šå¾Œç«¯
params.gpu_device = 0;      // GPU è¨­å‚™ ID

SimpleNeuralModel model(params);
```

### æ€§èƒ½èª¿å„ªå»ºè­°

1. **æ‰¹æ¬¡å¤§å°**: å¢åŠ æ‰¹æ¬¡å¤§å°å¯ä»¥æé«˜ GPU åˆ©ç”¨ç‡
2. **è¨˜æ†¶é«”ç®¡ç†**: é‡è¤‡ä½¿ç”¨è¨ˆç®—åœ–ä»¥æ¸›å°‘è¨˜æ†¶é«”åˆ†é…
3. **æ•¸æ“šé¡å‹**: è€ƒæ…®ä½¿ç”¨ FP16 ä»¥ç¯€çœè¨˜æ†¶é«”ï¼ˆéœ€è¦ç¡¬é«”æ”¯æ´ï¼‰
4. **ç•°æ­¥åŸ·è¡Œ**: ä½¿ç”¨ç•°æ­¥ API é‡ç–Šè¨ˆç®—å’Œæ•¸æ“šå‚³è¼¸

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

### é–‹ç™¼ç’°å¢ƒè¨­ç½®
```bash
git clone <your-repo>
cd llama.cpp
git submodule update --init --recursive
```

### ä»£ç¢¼é¢¨æ ¼
- ä½¿ç”¨ C++17 æ¨™æº–
- éµå¾ªç¾æœ‰çš„å‘½åæ…£ä¾‹
- æ·»åŠ é©ç•¶çš„è¨»é‡‹å’Œæ–‡æª”

## ğŸ“„ è¨±å¯è­‰

æœ¬é …ç›®éµå¾ª MIT è¨±å¯è­‰ã€‚è©³è¦‹ LICENSE æ–‡ä»¶ã€‚

## ğŸ™ è‡´è¬

- [GGML](https://github.com/ggerganov/ggml) - æ©Ÿå™¨å­¸ç¿’å¼µé‡åº«
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLaMA æ¨¡å‹æ¨ç†å¼•æ“ 